## CockroachDB and Kafka
CockroachDB is capable of acting as a Kafka producer that publishes messages to
Kafka topics in relation to database events. As this lab uses the CockroachDB
Cloud, you will need to configure the Kafka (running in the OpenShift cluster)
to allow external, authenticated communication.

## Configuring Kafka for External Communication
When the lab environment was set up, the Kafka cluster was exposed externally
via an OpenShift Route. The route was secured using TLS encryption and
communicates as HTTPS over port 443 (the default).

To get the route for the Kafka bootstrap cluster, you have two options.

### Option 1: OpenShift GUI
. Click the _Project_ dropdown at the top of the _Topology_ area and choose the `crdb-kafka` project
. Click _Project_ in the left-hand navigation
. Click _Routes_ in the _Inventory_ area
. Find the _Location_ for the route called `crdb-cluster-kafka-external-bootstrap`

### Option 2: OpenShift CLI
The following command will get the route information:

[source,bash,role=execute]
----
oc get route crdb-cluster-kafka-external-bootstrap -n crdb-kafka
----

Because the URL is so long, the line wrapping makes things look a little messy.
The important details are the following:

* the hostname, which ends in .com in our case
* the termination type, which is _passthrough_, meaning that the HAProxy load
balancer handling the external connections to OpenShift will pass the TLS
connection through to the endpoint (Kafka, in this case) without re-encrypting
or de-encrypting

## Obtaining the TLS certificate
CockroachDB, like most TLS-enabled applications, needs the certificate in order
to properly communicate. 

While you could also find the certificate by digging in the OpenShift web
console, it's fairly quick to do it via the CLI:

[source,bash,role=execute]
----
oc describe kafka crdb-cluster -n crdb-kafka
----

You will see something like:

----
Name:         crdb-cluster
Namespace:    crdb-kafka
Labels:       <none>
Annotations:  <none>
API Version:  kafka.strimzi.io/v1beta2
...
...
    Certificates:
      -----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUcFEfRknbqi5qbQnelpfNjKEQV4cwDQYJKoZIhvcNAQEN
...
----

## Set up the CockroachDB changefeed
Setting up the changefeed for your CockroachDB Cloud instance to talk to the
Kafka broker is as easy as an SQL command. Fortunately, we have provided a small
bash script that will save you the trouble of lots of copying and pasting:

[source,bash,role=execute]
----
cat ~/assets/cockroach-changefeed.sh
----

When you run the above scriptlet, the route and certificate details will be
fetched and then passed to the CockroachDB CLI. Just copy and paste the password
shown.

[source,bash,role=execute]
----
bash ~/assets/cockroach-changefeed.sh
----

## Verify the CockroachDB changefeed
Reconnect to the database using the service binding test script:

[source,bash,role=execute]
----
bash ~/assets/test-service-binding.sh
----

Once connected, the following SQL will tell you the status of any changefeed
configuration:

[source,sql,role=execute]
----
show changefeed jobs;
----

## Test the CockroachDB changefeed via SQL
Make sure you have the Python consumer application's logs showing:

. In the OpenShift web console, ensure you are using the _Developer_ perspective
. Make sure that you are in the _Topology_ view
. Select your `%user%-eda` project from the dropdown at the top
. Click the Python consumer application pod
. Click the pod name in the right hand navigation, which looks like
`python-kafka-consumer-696d6fbfbb-cfxw8`
. Click the _Logs_ tab

Now, in the terminal, check which fruits are available in your database:

[source,sql,role=execute]
----
select * from fruit limit 5;
----

You will see output like the following:

[source]
----
                   id                  |   name    | quantity | description
---------------------------------------+-----------+----------+--------------
  51661376-0a07-449b-a3bd-9db79cd4ead4 | Apple     |        0 | NULL
  69f6cd81-59fc-493b-8ebf-1b9f150ecead | Blueberry |        0 | NULL
  d37f4fae-b572-47b3-93e0-17daf798f9d5 | Banana    |        0 | NULL
----

Next, run an `UPDATE SQL` command on your outbox table. Make sure you use a row
ID from _your_ database. _DO NOT_ copy and paste one of the IDs from the example
above.

[source,sql,role=copy]
----
UPDATE fruit-outbox SET quantity = '50' where id = '<paste-any-rowid>';
----

If you look now at the Python consumer application's log window, you should see
an update was sent by CockroachDB to the Kafka broker, which was then consumed
and displayed in the application log.

## Test the changefeed with the Fruit application
Now that you have verified the changefeed via direct SQL commands, it's time to
do the full end-to-end test. You will probably want to open a new window into
the OpenShift web console so that you can keep the logs in view.

Alternatively, you could watch the Python consumer application's logs via the
OpenShift command line:

[source,bash,role=execute]
----
oc logs -f `oc get pod -l app=python-kafka-consumer -o jsonpath='{.items[0].metadata.name}'`
----

[NOTE]
You could have simply done an `oc get pod` to determine the name of the
consumer's pod, and then done `oc logs -f <podname>` - but the scriptlet is more
convenient!

Return to the Fruit application by clicking its route button in the _Topology_
view of your project:

Manipulate some fruit, and then watch the Python consumer application. You
should see that changes are flowing through to it via Kafka!
